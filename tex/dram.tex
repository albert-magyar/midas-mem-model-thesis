\section{DRAM Device Architecture}
In a DRAM IC, arrays of bit cells are hierarchically arranged into multiple
parallel \emph{banks}. Banks provide the primitive level of concurrency in a
DRAM memory system. They can service independent requests, assuming they do not
simultaneously require shared resources like the data, address and command
buses.  Multiple DRAM ICs can be arranged in parallel to widen the data bus;
address and command buses fan out to each IC.

A basic DRAM operation requires a series of three commands: \emph{activate
(ACT)}, \emph{column access (CAS)}, and \emph{precharge (PRE)}. The ACT
command enables the word-lines of the array corresponding to a single
\emph{row} of the bank. The cells of the row are sensed and saved in a
\emph{row buffer}(typically O(1) kB). A CAS command then selects a subset of
the row buffer to read or write; data is bursted over successive clock edges.
While the row buffer remains \emph{open}, the row can be accessed by issuing
new CAS commands. To access a row not stored in the buffer, a PRE command must
be issued to \emph{close} the row and charge the bit-lines for a new access.

\section{DRAM Controller Architecture}

At the highest level, a DRAM controller is responsible for responding to memory
requests from one or more requestors by scheduling those requests over its
attached memories as a judicious stream of DRAM commands.

Memory access scheduling (MAS) is the process by which, for a given cycle, a
controller selects a single DRAM command to be issued from a legal set. Legal
commands are constrained by the current state of each bank, the availability
of shared resources like the command and data buses, and timing constraints
imposed by the DRAM standard. Good MAS policies strike a delicate balance
between minimizing latency, maintaining quality-of-service guarantees across
multiple threads of execution, maximizing bandwidth, and minimizing power.
There are plethora of academic papers on MAS policies, and still more
industrial patents on the subject. We outline some of the most popular MAS policies
here.

\subsection{First-Come First-Serve (FCFS) Policy}\label{fcfs}
Commands for the oldest pending memory reference are always issued first. This
is the simplest MAS policy, but one that grossly under-utilizes available DRAM
bandwidth. FCFS schedulers are common in older machines, and those that present
few concurrent memory requests.

\subsection{First-Ready FCFS (FR-FCFS)\cite{frfcfs} Policy}\label{frfcfs}
First, column access commands that hit in an open row-buffer are prioritized,
then row and precharge commands from the oldest pending reference are
considered.  FR-FCFS is a relatively simple scheme that achieves far higher BW
than FCFS. It is the defacto standard against which new MAS policies for
machines with a single stream of memory references (like single-core
out-of-order machines), are compared.

\section{System Integration of Off-Chip Memory Systems}


\section{DRAM Software Simulation}

The current state of the art in DRAM simulation in academia are cycle-accurate
software simulators like DRAMSim2\cite{dramsim}, Ramulator\cite{ramulator} and
USIMM\cite{usimm}. These models generate DRAM command streams that have been
validated against industrial models (for some standards). Both Ramulator and
DRAMSim2 can be easily integrated into Gem5\cite{gem5}, though it includes a
detailed event-based model of its own\cite{gem5event}. In trace-driven mode,
operating at full throughput and only as a timing model, these cycle-accurate
models simulate at frequencies ranging from 100s of KHz to ones of
MHz\cite{ramulator}.

\section{DRAM Power Modeling}

In order to model power, Ramulator relies on DRAMPower\cite{drampower}, to
which it passes its command trace. Micron describes strategies in their
technical notes for estimating DRAM power\cite{micronpower} (which DRAMSim2
employs). They also made these calculations accessible by providing
spreadsheets that take micro-architectural event frequencies as input. These
approaches can carry over to FPGA simulation, as sufficiently detailed FPGA
models can add instrumentation for these events, while simpler models can save
the memory access trace to a buffer and compute power
out-of-band\cite{strober}.
