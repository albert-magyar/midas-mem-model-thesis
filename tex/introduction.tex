For the past half century, transistor scaling has delivered incredible
improvements in computing performance and energy efficiency. The rapid advance
of process technology has proven to be a strong disincentive to building ASICs,
as the competitive advantage of a custom-silicon design would usually be lost
to a general-purpose machine built in a newer process technology. This coupled
with famously large and growing NRE, has limited ASIC deployment to high volume
domains where they offer a considerable advantage over a general-purpose part.
For applications that would see enormous improvements with custom-silicon but
lack the volume to overcome the NRE, FPGAs have proven to be an effective
stopgap, despite being inferior to ASICs in logic density, delay, and energy
efficiency~\cite{fpgagap}.

With Moore's law ending, the risk balance for system architects is beginning
to change: while NRE remains high, custom-silicon may often present a lasting
competitive advantage.  Application, compiler, and OS developers will be more
inclined to support specialized devices if they can deliver a performance
improvement that cannot foreseeably be met by a general purpose machine.
Perhaps the greatest performance and energy efficiency improvements are to be
had when hardware and the software can be \emph{co-designed}.  In the absence
of frequency scaling, custom-silicon and hardware-software co-design are
perhaps the only ways to maintain the advances in computing performance and
efficiency advances the market has come to expect.

NRE, however, remains a substantial barrier to custom-silicon.
Increasingly, however, the largest contributor to NRE is that of software
development.  Ideally, software development would proceed in parallel to
hardware development, however, lacking fast, cycle-accurate \emph{full-system
simulators} (simulators capable of executing the entire software stack),
software engineers are forced to develop against a inaccurate model of the SoC
or wait until first silicon is back. This precludes
effective hardware-software co-design, as it difficult to reason about
full-system design trade-offs if the software and hardware are being evaluated
in different simulation environments. This lack of fast and accurate
full-system simulation is often referred to as the \emph{simulation gap}.

To address the simulation gap, many have turned to FPGAs. In industry,
companies have long relied on \emph{FPGA prototypes} that implement the SoC's
RTL directly. In academia, researchers have explored using FPGAs as \emph{accelerators}
for architecture simulations~\cite{fast, fame, hasim, protoflex,ramp} that are
better suited for prototyping research ideas and performing design space
exploration. Despite early promise, \emph{FPGA-accelerated simulation} has seen
little adoption in the academia or industry for a number of reasons: 1)
FPGA-accelerated simulators are difficult to write or modify, 2) FPGA mappings
require a lengthy compilation time, 3) FPGAs are difficult to debug, 4) FPGAs
have historically been resource constrained, either limiting the scale of the
system under simulation or requiring even more complexity to partition designs
across FPGAs, and 5) FPGA hardware and software tools are expensive to purchase
and maintain.  Recent technological advances have eased some of these
challenges. FPGAs are larger and faster than ever before and are becoming
available as resources in research clusters~\cite{catapultannounce} and
datacenters~\cite{amazonf1}.  Given the lack of a competing technology that is
both as cheap and as effective at accelerating cycle-accurate simulations, it
appears that FPGAs remain the only vehicle capable of bridging the simulation
gap, and thus demand continued investigation.

\emph{MIDAS} (Modeling Infrastructure for Debugging and Simulation) is Berkeley
Architecture Research's answer to improving the usability of FPGA-accelerated
simulators. MIDAS permits co-hosting software models and FPGA-accelerated
models on an arbitrary \emph{host-platform} consisting of a mix of FPGAs and
CPUs.  During early design space exploration or system specification, a MIDAS
simulation may consist entirely of software models of the system.  However, as
RTL implementations become available, MIDAS can automatically transform them
into FPGA-accelerated models that can be linked into the simulator. Throughout
the whole process there is a functioning model of the machine that is fast
enough for software development.

One challenge with this vision is that there are many cases in which an
FPGA-accelerated model is required but there is not yet RTL available to be
transformed. Here, handwritten-RTL models are required but are subject to the same
pitfalls of prior FPGA-accelerated simulation work. For MIDAS to
succeed, models like this must be flexible enough to be reusable across a wide
range of simulations in order to amortize the high cost of writing them.
Generally, we propose that using a hardware \emph{generator} to synthesize
timing-model \emph{instances} can address this challenge. To demonstrate this, the
generator described herein synthesizes models for an important component of
nearly all computer systems: DRAM memory systems. Model instances use the
available FPGA off-chip memory system as a backing store for a timing-model
hosted in FPGA fabric. Model instances are reconfigurable at runtime, allowing for
design space exploration without lengthy FPGA recompilation. Instances can be
generated with instrumentation to ease debugging and to perform non-invasive
measurements of the simulation. Finally, since the in-fabric timing-model is
split from the functional components that interact with the FPGA memory system,
extending the generator with new timing-model is easy.

This report builds on the work of an earlier master's thesis by Asif
Khan~\cite{khanmasters} addressing challenge of modeling off-chip memory
systems in RAMP~\cite{ramp}. This report differs from that work in its use of
a generator, in its demonstration of the model interoperating with models
automatically transformed from source RTL, and in the fidelity at which
DRAM memory systems are modeled.

\section{Collaboration, Previous Publications, and Funding}

This report builds on the work of current and previous students of our group.
The MIDAS project is a multi-student collaboration between myself, Jack Koenig,
Sagar Karandikar, and Donggyu Kim, whose Chisel3 port of Strober~\cite{strober}
constitutes the bulk of the MIDAS code base at the time of writing. The
generator presented here and the RTL libraries used in MIDAS are written in
Chisel3~\cite{chisel} which remains under active development. MIDAS leans
heavily on FIRRTL~\cite{firrtl} to transform source RTL into FPGA-accelerated
models.  Finally, MIDAS depends on the availability of open-source SoC IP as a
source of transformable RTL. Here we use the Rocket-Chip~\cite{rocketchip} SoC
generator and BOOM~\cite{boom} Out-of-Order~(OoO) core generator which
implement the free and open RISC-V ISA~\cite{riscv}.

Work on this memory model began in the fall of 2015, as a course project for
CS252 for which Jack Koenig was my partner. Thereafter, development of the
memory model was driven by MICRO submissions attempts in 2017 and 2018. For the
2017 iteration we had only nascent forms of the DDR models presented herein. I
have my co-authors, Jack Koenig, Chris Celio and especially Martin Maas, who
engineered the DaCapo on Jikes case study, Donggyu Kim, who was reponsible for
running most of the experiments and instrumenting MIDAS so as to make the case
study possible, to thank for getting that half-baked submission out the door.
It proved to be an invaluable source of feedback.  In 2018, FireSim had just
been accepted to ISCA, and Sagar had finished rewriting the manager, which made
collecting the results for the evaluation of this report possible. We'd just
reorganized FireSim, and other repos using MIDAS as a library --- Donggyu Kim
play a key role in getting everything running again.  Howard Mao wrote the
block device and maintained much of the system software.  Jack Koenig wrote
many of the plotting scripts used in the evaluation, and made an eleventh hour
push to get command-trace driven power analysis working, which ultimately
didn't make the paper. Finally, Andrew Waterman had instrumental role in
editing the paper~(and this report) and really helped me keep the whole
operation on track.

\TODO{Funding}
