\chapter{Introduction}

For the past half century, Moore's law, and Dennard's law before it, has
delivered incredible improvements in computing performance. The rapid advance
of process technology has proven to be a strong disincentive to building custom
silicon, as any competitive advantage achieved by a
custom design in a particular node would be lost to a general-purpose
machine in a newer node. Consequently, advances in computing have
been focused broadly about making programmable, general-purpose machines
faster, by aggressively pursuing ILP in CPUs, and DLP in GPUs, DSPs. For
applications that might otherwise see orders of magnitude improvement with
custom silicon over programmable solutions, but lack the volume to overcome the
NRE costs, FPGAs have proven to be an effective stopgap -- by trading an order of
magnitude in cost for an order of magnitude in performance that comes at the
expense, of again, programmabililty

With the death of Dennard scaling, and apparent end of Mooreâ€™s law, future
advances in computing performance must come from improvements above the process
technology.  For designers the risk balance has changed: custom silicon may now
present a lasting competitive advantage. Application developers will inclined
to code against specialized computing devices -- perhaps, at the expense of
code portability -- if it can deliver a performance improvement that will not
be met by a general purpose machine within the lifetime of the product. It is
not difficult to envision a future in which custom silicon designs, each with
relatively low volumes, account for the bulk of semiconductors manufactured, if
custom silicon were to become more accessible.

% Talk about open hardware

Of course, custom silicon is famously inaccessible. The NRE of a new design in
the latest process technology is often quoted as being upwards of \$100
million.  There are many contributing factors, including mask
costs, CAD tool costs, IP costs, and engineering time for design and
verification. Necessarily, all of these costs must fall, if custom silicon is
to have a future in all but the largest corporations.

\section{Attacking Full-System Simulation}

One key driver of costs in the ASIC design process is simulation. Simulation preforms
three different functions.

\begin{enumerate}

    \item Prototyping: "What thing should we build?" Serves as a means to
        rapidly evaluate different design points with abstract models, or an
        incomplete implementation of a proposed design.

    \item Verification: "Did we build the thing right?" To check that a
        particular implementation has correctly executes.

    \item Validation: "Did we build the right thing?" To show that an
        implementation fulfills the high-level objectives set out for the
        system. Here the hope is that simulation of the implementation
        "validates" the results of the prototype.

\end{enumerate}

The core of the design and development process of an ASIC are a suite of
simulation technologies, ranging from SPICE simulation of analog circuits,
discrete event simulation of RTL implementations, to architectural simulators
that may do no accounting of time.  Each of these technologies makes a tradeoff
between \textit{fidelity}~(how accurate is the simulation), \textit{speed}~(how
quickly does simulator execute) and \textit{cost}~(\$ per simulation hour).


What should then follow is a concretization the system -- as parts of the
simulation are improved with more accurate models or implementations as they
become available. The challenge is that until that silicon is back -- the speed
of those simulations slows down exponentially as it approaches implementation.
While it may have been possible to run a whole benchmark suite (like SPEC) with
an ISA simulator in a single day, running the same suite could take months in
an abstract cycle-accurate simulator like GEM5, or years in a full-system RTL
simulation[]. Thus it is only until first silicon is back that the designer can
run the complete workload on the design again. The gap between the speed of
full system simulation and silicon, forces teams to resort to FPGA
prototyping[], or specialized hardware emulation accelerators like Synopsys
palladium\cite{palladium}.

The focus of this work, and the larger of body of work to which it belongs.
amounts to bridging this gap in full-system simulation. Broadly, this achieved
by presenting the right simulation abstractions to developer, that
simultaneously permits co-hosting SW models and FPGA-accelerated RTL models. As
implementations, or more complete models of components of the system become
available, they can replace more abstract ones. Through the whole process there
is a functioning model of the target machine, that is fast enough for
applications developers to code against.

Specifically, this work presents an important component of such a simulation
environment: an FPGA-hosted off-chip memory model, whose timing model is both
reconfigurable and agnostic of the particular FPGA architecture that hosts it.




