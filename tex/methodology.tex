\section{Target Design Configuration}

To demonstrate of the use of our generator, we evaluated two
microarchitecturally different processors:  Rocket, a 5-stage single-issue
in-order core, and BOOM, a superscalar out-of-order core~\cite{boom}. Both
microarchitectures leverage the open-source Rocket-Chip SoC
generator~\cite{rocketchip}. As described in section~\ref{sec:targetandhostmachines}, all of the instances of Rocket Chip used in
this report have same top-level I/O and are modeled as part of a target with
the MIDAS SDF network shown in figure~\ref{fig:default-target}.

Both cores implement the 64-bit scalar RISC-V ISA~\cite{Waterman:EECS-2016-118,
Waterman:EECS-2016-161}\footnote{User level specification v2.1, privileged
specification v1.9} which includes support for atomics, IEEE 754-2008
floating-point, and page-based virtual memory (RV64IMAFD).
Table~\ref{tbl:target} lists the selected configurations of the target
processors.\footnote{Note that BOOM-2w roughly approximates the configuration
of the ARM Cortex-A9 processor, which coincidentally is same microarchitecture
embedded in the Zynq host.}. Unfortunately, neither configuration includes an
L2 cache as it was  temporarily removed in the version of Rocket Chip used to
conduct the evaluation. As a final note, the Rocket configuration we used
includes a non-blocking data cache (instead of the default blocking
implementation). Unlike a classic RISC pipeline, Rocket's backend includes a
scoreboard that allows it to make multiple memory requests when there are no
intermediate dependent instructions.

\begin{table}
\begin{center}
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{|r|c c|}
    \hline
     & \textbf{Rocket} & \textbf{BOOM-2w} \\
    \hline
    \textit{Fetch-width} & 1 & 2 \\
    \textit{Issue-width} & 1 & 3 \\
    \textit{Issue slots} & - & 16 \\
    \textit{ROB size} & - & 48 \\
    \textit{Ld/St entries} & - & 16/16 \\
    \textit{Physical registers} & 32(int)/32(fp) & 110 \\
    \textit{Branch predictor} & - & tage \\
    \textit{MSHR entries} & 2 & 6 \\
    \textit{L1 I\$ and D\$} & \multicolumn{2}{c|}{16KiB / 16KiB} \\
    \textit{ITLB and DTLB reaches} & \multicolumn{2}{c|}{128KiB / 128KiB} \\
    \hline
\end{tabular}}
\end{center}
\caption{Processor Parameters}
\label{tbl:target}
\end{table}%

%\begin{table*}
%\centering
%	\begin{tabular}{|c|S[table-format=5]|c|S[table-format=5]|c|S[table-format=5]|c|S[table-format=5]|c|}
%	\hline
%	& \multicolumn{2}{c|}{\textbf{Latency-Bandwidth Pipe}} & \multicolumn{2}{c|}{\textbf{Bank Conflict Model}} & \multicolumn{2}{c|}{\textbf{FIFO MAS}} & \multicolumn{2}{c|}{\textbf{FR-FCFS}} \\
%	\hline
%	\textbf{Resource} & \textbf{Utilization} & \textbf{\%} & \textbf{Utilization} & \textbf{\%} & \textbf{Utilization} & \textbf{\%} & \textbf{Utilization} & \textbf{\%} \\
%	\hline
%	\textit{LUT} & 3586 & 1.6 & 4170 & 1.9 & 3812 & 1.7 & 6822 & 3.1 \\
%	\textit{LUTRAM} & 651 & 0.9 & 607 & 0.8 & 655 & 0.9 & 693 & 1.0 \\
%	\textit{FF} & 3146 & 0.7 & 3929 & 0.9 & 3426 & 0.8 & 5282 & 1.2 \\
%	\textit{BRAM} & 11 & 2.0 & 11 & 2.0 & 11 & 2.0 & 11 & 2.0 \\
%	\hline
%	\end{tabular}
%
%\caption{Resource utilization of various memory system models for Xilinx zc706 FPGA}
%\label{tbl:utilization}
%\end{table*}

\section{System Software}

Unless otherwise stated, all benchmarks were run on Linux (kernel version
4.6.2). Unfortunately, at time of writing we do not have the ability to proxy a
block device requests over the tether (as is done with I/O). Thus for each
workload, we built a minimal BusyBox image and included all required files for
that workload within the image's initramfs.  Generally, workloads were compiled
statically (using \texttt{gcc -02}). When required, namely for the Java workloads, we
built library dependencies using the Yocto (\texttt{riscv-poky}) Linux
distribution generator. After packaging up the complete Linux image, it is
built into an instance of the Berkeley Bootloader (BBL).

Despite our best efforts, this process produces relatively large BBL instances
($>$ 10MiB), which are slow to load directly over the tether.  We modified
\texttt{riscv-fesvr} to permit the MIDAS master to detect and accelerate program load
out-of-band (instead of fully simulating the process over the tether).

Finally, present limitations regarding how console I/O is proxied to
\texttt{riscv-fesvr} can dramatically slow down simulation depending on the
volume of target console I/O. \footnote{Changes introduced to
\texttt{riscv-fesvr} with the privileged specification 1.10 updates remove this
bottleneck, as the host is no longer is required to acknowledge console
output.} To circumvent this, all console output is piped to a file on the
target's filesystem. At the end of the workload, the file is dumped to the
console, and \texttt{riscv-fesvr} enters a custom fast I/O mode.  Thus, low FMR
is maintained without perturbing program execution. In our evaluation flow, no
console input is ever presented to the target; the \texttt{init} script of the
Linux image includes all of the commands required to run and measure the
desired workload, after which it powers down the machine (See listing~\ref{lst:init} for an example).\\

\lstdefinestyle{init}{
    frame=lines,
    captionpos=b,
    %xleftmargin=\parindent,
    language=bash,
    showstringspaces=false,
    basicstyle=\footnotesize\ttfamily,
    %keywordstyle=\bfseries\color{green!40!black},
    commentstyle=\itshape\color{blue},
    identifierstyle=\color{black},
    stringstyle=\color{orange},
}

\lstset{style=init}

\begin{lstlisting}[caption={An example init script generated during the build process},label={lst:init}]
    cd /biancolin
    # Periodically polls HPM counters to measure target events
    /biancolin/rv_counters/rv_counters >> dump &
    sleep 1

    # Run the workload
    ./hello >> dump 2>&1

    # Cleanup, kill the counter program, and cat results to console
    killall rv_counters
    while pgrep rv_counters > /dev/null; do sleep 2; done
    sync
    cat dump
    poweroff -f
\end{lstlisting}

\section{Measuring the Target}

Measurements of the target design occur in two places.
Memory timing-model instances collect their own statistics, and
as described previously, are read with MMIO through the simulation
interconnect. While these measurements can be made while the simulation is
executing, in our evaluation we halt target execution to get a consistent
snapshot. Regardless, these measurements do not perturb simulation behavior.

While we ultimately plan to support transformations that permit
measuring generated-RTL models non-invasively, in this report,
measurements of the core are made using a process running on the target machine,
\texttt{rv\_counters}. This program wakes up every hundred million target
cycles to read the core's hardware performance monitor (HPM) which by default measures
cycles and instructions retired. Additionally, the HPM provides 29 other counters to measure
events defined by the microarchitect like branch mispredictions or data
cache misses (see~\cite{Waterman:EECS-2016-161} section 3.1.15).
\texttt{rv\_counters} prints the values of these counters to standard out; they are collected
in a second file in the target filesystem. Since polling events are both
short-lived and infrequent, perturbations in target execution are insignificant.

When a specific event was not measured by the existing design, we added them
manually to Rocket-Chip. In addition to measuring conventional
microarchitectural events, with this mechanism we were also able to measure the
execution time of regions of the application, like memory allocations in the
JVM, with effectively zero perturbation. To do this, we instrumented the target
program with specially encoded NOP instructions which would enable and disable
an incrementer when decoded.

\section{Workloads}

To demonstrate that our platform allows us to perform high-fidelity experiments
involving realistic software applications without sub-sampling execution, we
chose two non-trivial benchmark suites, the SPEC 2006 CPU
benchmarks~\cite{spec_cpu_2006} and the DaCapo benchmarks~\cite{dacapo}.

\subsection{SPECint2006 Benchmarks} The SPECint2006 benchmarks are de facto
standard to benchmarking sequential performance of general-purpose
microprocessors, and widely used by industrial computer architects to validate
their designs. Academic computer architects seldom run the complete
``reference" workload due to their length which commit between 23-25 trillion
dynamic instructions in RISC-V (RV64G).  It would take at least eight days to run
the test workloads on a single instance of a very fast microarchitectural
cycle-accurate software simulator (400~KIPS)~\cite{marssx86}, and nearly two
years to run the reference workloads. For comparison, a single MIDAS simulator,
assuming ($f_{fpga} = 40 MHz, FMR = 1.25, CPI_{target} = 2)$ takes
approximately 5 hours and 17 days to run test and reference workloads
respectively.

%\begin{table}[t]
%	\begin{center}
%    \resizebox{0.6\textwidth}{!}{
%        \begin{tabular}{|c|S[table-format=3.1]|S[table-format=3.1]|}
%        \hline
%        \textbf{Benchmarks} & \textbf{Instructions~(B)} & \textbf{MPKI} \\
%        \hline
%		\textit{400.perlbench} & 2.6 & 49.5 \\
%		\textit{401.bzip2} & 34.4 & 27.5 \\
%		\textit{403.gcc} & 5.4 & 37.2 \\
%		\textit{429.mcf} & 3.5 & 274.6\\
%		\textit{445.gobmk} & 66.4 & 36.4 \\
%		\textit{456.hmmer} & 16.5 & 4.3 \\
%		\textit{458.sjeng} & 18.9 & 23.1 \\
%		\textit{462.libquantum} & 0.3 & 32.0 \\
%		\textit{464.h264ref} & 104.2 & 7.5 \\
%		\textit{471.omnetpp} & 1.9 & 61.0 \\
%		\textit{473.astar} & 22.8 & 45.8 \\
%		\textit{483.xalancbmk} & 0.4 & 64.3 \\ 
%		\hline
%		\end{tabular}
%		\label{tbl:spec_test}
%    }%
%	\caption{Dynamic instruction counts and MPKI for the SPECint2006 benchmarks with test inputs}
%    \vspace{0.2cm}
%
%    \resizebox{0.6\textwidth}{!}{%
%        \begin{tabular}{|c|S[table-format=3.1]|S[table-format=3.1]|}
%        \hline
%        \textbf{Benchmarks} & \textbf{Instructions~(B)} & \textbf{MPKI} \\
%        \hline
%		\textit{403.gcc} & 1366.5 & 54.2 \\
%		\hline
%		\end{tabular}
%    }%
%	\end{center}
%    \caption{Dynamic instruction count and MPKI for SPECint2006 gcc with its reference input (all workloads)}
%	\label{tbl:spec}
%\end{table}

\subsection{DaCapo Benchmarks}

%\begin{table}
%\begin{center}
%\resizebox{0.6\textwidth}{!}{%
%	\begin{tabular}{|c|S[table-format=3.1]|S[table-format=3.1]|}
%	\hline
%	\textbf{Benchmarks} & \textbf{Instructions~(B)} & \textbf{MPKI} \\
%	\hline
%	\textit{avrora} & 216.5 & 51.2 \\
%	\textit{luindex} & 72.5 & 30.5 \\
%	\textit{lusearch} & 131.5 & 34.1 \\
%	\textit{pmd} & 109.6 & 34.1 \\
%	\textit{sunflow} & 124.5 & 41.6 \\
%	\textit{xalan} & 152.9 & 37.9 \\
%	\hline
%	\end{tabular}
%}%
%\end{center}
%\caption{Dynamic instruction counts and MPKI for the DaCapo benchmarks with the ``small'' input size}
%\label{tbl:dacapo}
%\end{table}

The DaCapo benchmarks are widely used Java benchmarks that represent full Java
applications, including the Lucene search engine and a raytracer. We use
version 9.12 of the benchmark suite and exclude the benchmarks that do not run
on recent versions of JikesRVM. Specifically, we run \emph{avrora},
\emph{luindex}, \emph{lusearch}, \emph{pmd}, \emph{sunflow} and \emph{xalan}.

For each benchmark, we ran one full pass of the ``small'' input size,
accounting for both class loading and Just-in-Time compilation. We believe that
this gives a comprehensive and realistic view of the full execution of a Java
program.

JikesRVM is configured to use the \emph{MarkSweep} garbage collector and the
default settings. MarkSweep is a non-relocating Mark \& Sweep Garbage Collector
with a segregated free-list allocator (meaning that it maintains a number of
free lists for different size classes of objects, with a shared pool of pages
that can be acquired and released by these free lists).
