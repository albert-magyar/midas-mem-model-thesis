\section{Target Design Configuration}

To demonstrate of the generator, we evaluate two microarchitecturally different
processors:  Rocket, a 5-stage single-issue in-order core, and BOOM (Berkeley
Out-of-Order Machine), a superscalar out-of-order core~\cite{boom}. Both
microarchitectures leverage the open-source Rocket-Chip SoC
generator~\cite{rocketchip}. All of the instances of RocketChip used in this
report have same top-level I/O: hence any instance can be transformed and
modeled as part of a target with the MIDAS SDF network shown in figure
\ref{fig:default-target} (in section (\ref{sec:targetandhostmachines}).

Both cores implement the 64-bit scalar RISC-V ISA~\cite{Waterman:EECS-2016-118,
Waterman:EECS-2016-161}\footnote{User level specification v2.1, privileged
specification v1.9} which includes support for atomics, IEEE 754-2008
floating-point, and page-based virtual memory (RV64IMAFD).
Table~\ref{tbl:target} lists the selected configurations of the target
processors.\footnote{Note that BOOM-2w roughly approximates the configuration
of the ARM Cortex-A9 processor, which coincidentally is same microarchitecture
embedded in the Zynq host.}. Unfortunately, neither configuration includes an
L2 cache as it was  temporarily removed in the version of RocketChip used to
conduct the evaluation. As a final note, the rocket configuration we used
includes a non-blocking data cache (instead of the default blocking
implementation). Unlike a classic RISC pipeline, rocket's backend includes a
scoreboard that allows it to make multiple memory requests when there are no
intermediate dependent instructions.

\begin{table}
\begin{center}
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{|r|c c|}
    \hline
     & \textbf{Rocket} & \textbf{BOOM-2w} \\
    \hline
    \textit{Fetch-width} & 1 & 2 \\
    \textit{Issue-width} & 1 & 3 \\
    \textit{Issue slots} & - & 16 \\
    \textit{ROB size} & - & 48 \\
    \textit{Ld/St entries} & - & 16/16 \\
    \textit{Physical registers} & 32(int)/32(fp) & 110 \\
    \textit{Branch predictor} & - & tage \\
    \textit{MSHR entries} & 2 & 6 \\
    \textit{L1 I\$ and D\$} & \multicolumn{2}{c|}{16KiB / 16KiB} \\
    \textit{ITLB and DTLB reaches} & \multicolumn{2}{c|}{128KiB / 128KiB} \\
    \hline
\end{tabular}}
\end{center}
\caption{Processor Parameters}
\label{tbl:target}
\end{table}%

%\begin{table*}
%\centering
%	\begin{tabular}{|c|S[table-format=5]|c|S[table-format=5]|c|S[table-format=5]|c|S[table-format=5]|c|}
%	\hline
%	& \multicolumn{2}{c|}{\textbf{Latency-Bandwidth Pipe}} & \multicolumn{2}{c|}{\textbf{Bank Conflict Model}} & \multicolumn{2}{c|}{\textbf{FIFO MAS}} & \multicolumn{2}{c|}{\textbf{FR-FCFS}} \\
%	\hline
%	\textbf{Resource} & \textbf{Utilization} & \textbf{\%} & \textbf{Utilization} & \textbf{\%} & \textbf{Utilization} & \textbf{\%} & \textbf{Utilization} & \textbf{\%} \\
%	\hline
%	\textit{LUT} & 3586 & 1.6 & 4170 & 1.9 & 3812 & 1.7 & 6822 & 3.1 \\
%	\textit{LUTRAM} & 651 & 0.9 & 607 & 0.8 & 655 & 0.9 & 693 & 1.0 \\
%	\textit{FF} & 3146 & 0.7 & 3929 & 0.9 & 3426 & 0.8 & 5282 & 1.2 \\
%	\textit{BRAM} & 11 & 2.0 & 11 & 2.0 & 11 & 2.0 & 11 & 2.0 \\
%	\hline
%	\end{tabular}
%
%\caption{Resource utilization of various memory system models for Xilinx zc706 FPGA}
%\label{tbl:utilization}
%\end{table*}

\section{System Software}

Unless otherwise stated, all benchmarks were run on Linux (kernel version
4.6.2). Unfortunately, at time of writing we do not have the ability to proxy a
block device requests over the tether (as is done with I/O). Thus for each
workload, we built a minimal BusyBox image and included all required files for
that workload within the image's initramfs.  Generally, workloads were compiled
statically (using \texttt{gcc -02}). When required, namely for the Java workloads, we
built library dependencies using the Yocto (\texttt{riscv-poky}) Linux
distribution generator. After packaging up the complete linux image, it is
built into an instance of the Berkeley Boot Loader (BBL).

Despite our best efforts, this process produces relatively large BBL instances
($>$ 10MiB), which are slow to load directly over the tether.  We modified
\texttt{FESVR} to permit the MIDAS master to detect and accelerate program load
out-of-band (instead of fully simulating the process over the tether).

Finally, present limitations with how console I/O is proxied over the tether to
\texttt{FESVR} can dramatically slow down simulation (FMR may be increased
tenfold or more) depending on the volume of target console I/O. To circumvent
this, all console output is piped to a file on the target's filesystem. At the
end of the workload, the file is \texttt{cat}-ed to the console, and FESVR
enters a custom fast I/O mode. Thus, low FMR is maintained without pertubing
program execution. In our evaluation flow, no console input is ever presented
to the target. The \texttt{init} script included in the linux image includes
all of the commands required to run and measure the desired workload,
afterwhich it powers down the machine (See listing \ref{lst:init}).\\

\lstdefinestyle{init}{
    frame=lines,
    captionpos=b,
    %xleftmargin=\parindent,
    language=bash,
    showstringspaces=false,
    basicstyle=\footnotesize\ttfamily,
    %keywordstyle=\bfseries\color{green!40!black},
    commentstyle=\itshape\color{blue},
    identifierstyle=\color{black},
    stringstyle=\color{orange},
}

\lstset{style=init}

\begin{lstlisting}[caption={An example init script generated during the build process},label={lst:init}]
    cd /biancolin
    # Periodically polls HPM counters to measure target events
    /biancolin/rv_counters/rv_counters >> dump &
    sleep 1

    # Run the workload
    ./hello >> dump 2>&1

    # Cleanup, kill the counter program, and cat results to console
    killall rv_counters
    while pgrep rv_counters > /dev/null; do sleep 2; done
    sync
    cat dump
    poweroff -f
\end{lstlisting}

\section{Measuring the Target}

Measurements of the target design occur through in two places.
Memory-timing-model instances collect their own memory system statistics, and
as described previously, are read with MMIO through the simulation
interconnect. While these measurements can be made while the simulation is
executing, in our evaluation we halt target execution to get a consistent
snapshot. In either case, these measurements do not perturb simulation
behavior.

While we ultimately plan to support source transformations that permit
measuring generated RTL models in a similar fashion, in this report,
measurements of the core are made invasively with a target process,
\texttt{rv\_counters}. This program wakes up every hundred million target
cycles to read the core's hardware performance monitor which natively measures
cycles and instructions retired, but provides 29 other counters to measure
other events defined by the microarchitect like branch mispredictions or data
cache misses (see \cite{Waterman:EECS-2016-161} section 3.1.15).
\texttt{rv\_counters} prints these values to standard out; they are collected
in a second file in the target filesystem.  Since polling events are both
short-lived and infrequent, perturbations in target execution are insignificant.

When a specific event type was not measured by the existing design, we added
them manually to the generator. In addition to measuring conventional
microarchitectural events, with this mechanism we were also able to measure the
execution time of short-lived events, like memory allocations in the JVM, with
effectively zero pertubation. To do this, we instrumented the target program
with specialy encoded NOP instructions which would enable and disable an
incrementer when decoded.

\section{Workloads}

To demonstrate that our platform allows us to perform high-fidelity experiments
involving realistic software applications without sub-sampling execution, we
chose two non-trivial benchmark suites, the SPEC 2006 CPU
benchmarks~\cite{spec_cpu_2006} and the DaCapo benchmarks~\cite{dacapo}.

\subsection{SPECint2006 Benchmarks} he SPECint2006 benchmarks are widely used
by computer architects to validate their designs. However, evaluations using
microarchitectural cycle-levelj software simulators typically do not run the
entire suite due to their length.

Table~\ref{tbl:spec} shows the dynamic instruction counts and the cache and TLB
misses per 1000 instructions~(MPKI) for each benchmark executed with its test
inputs. Even with the test inputs, it would take at least 8 days to run the
whole set of benchmarks on a single instance of a very fast microarchitectural
cycle-accurate software simulator (400~KIPS)~\cite{marssx86}.

\begin{table}[t]
	\begin{center}
    \resizebox{0.6\textwidth}{!}{
        \begin{tabular}{|c|S[table-format=3.1]|S[table-format=3.1]|}
        \hline
        \textbf{Benchmarks} & \textbf{Instructions~(B)} & \textbf{MPKI} \\
        \hline
		\textit{400.perlbench} & 2.6 & 49.5 \\
		\textit{401.bzip2} & 34.4 & 27.5 \\
		\textit{403.gcc} & 5.4 & 37.2 \\
		\textit{429.mcf} & 3.5 & 274.6\\
		\textit{445.gobmk} & 66.4 & 36.4 \\
		\textit{456.hmmer} & 16.5 & 4.3 \\
		\textit{458.sjeng} & 18.9 & 23.1 \\
		\textit{462.libquantum} & 0.3 & 32.0 \\
		\textit{464.h264ref} & 104.2 & 7.5 \\
		\textit{471.omnetpp} & 1.9 & 61.0 \\
		\textit{473.astar} & 22.8 & 45.8 \\
		\textit{483.xalancbmk} & 0.4 & 64.3 \\ 
		\hline
		\end{tabular}
		\label{tbl:spec_test}
    }%
	\caption{Dynamic instruction counts and MPKI for the SPECint2006 benchmarks with test inputs}
    \vspace{0.2cm}

    \resizebox{0.6\textwidth}{!}{%
        \begin{tabular}{|c|S[table-format=3.1]|S[table-format=3.1]|}
        \hline
        \textbf{Benchmarks} & \textbf{Instructions~(B)} & \textbf{MPKI} \\
        \hline
		\textit{403.gcc} & 1366.5 & 54.2 \\
		\hline
		\end{tabular}
    }%
	\end{center}
    \caption{Dynamic instruction count and MPKI for SPECint2006 gcc with its reference input (all workloads)}
	\label{tbl:spec}
\end{table}

\subsection{DaCapo Benchmarks}

\begin{table}
\begin{center}
\resizebox{0.6\textwidth}{!}{%
	\begin{tabular}{|c|S[table-format=3.1]|S[table-format=3.1]|}
	\hline
	\textbf{Benchmarks} & \textbf{Instructions~(B)} & \textbf{MPKI} \\
	\hline
	\textit{avrora} & 216.5 & 51.2 \\
	\textit{luindex} & 72.5 & 30.5 \\
	\textit{lusearch} & 131.5 & 34.1 \\
	\textit{pmd} & 109.6 & 34.1 \\
	\textit{sunflow} & 124.5 & 41.6 \\
	\textit{xalan} & 152.9 & 37.9 \\
	\hline
	\end{tabular}
}%
\end{center}
\caption{Dynamic instruction counts and MPKI for the DaCapo benchmarks with the ``small'' input size}
\label{tbl:dacapo}
\end{table}

To demonstrate how our platform enables studies that are difficult to perform
in existing FPGA-based simulators and cycle-accurate software simulators, we
run the DaCapo benchmarks on JikesRVM, a research Java Virtual Machine that is
widely used in managed-language research.

The DaCapo benchmarks are widely used Java benchmarks that represent full Java
applications, including the Lucene search engine and a raytracer. We use
version 9.12 of the benchmark suite and exclude the benchmarks that do not run
on recent versions of JikesRVM. Specifically, we run \emph{avrora},
\emph{luindex}, \emph{lusearch}, \emph{pmd}, \emph{sunflow} and \emph{xalan}.

For each benchmark, we ran one full pass of the ``small'' input size,
accounting for both class loading and Just-in-Time compilation. We believe that
this gives a comprehensive and realistic view of the full execution of a Java
program. Figure~\ref{tbl:dacapo} shows the dynamic instruction counts and MPKI
for each benchmark with the ``small'' input size.

JikesRVM is configured to use the \emph{MarkSweep} garbage collector and the
default settings. MarkSweep is a non-relocating Mark \& Sweep Garbage Collector
with a segregated free-list allocator (meaning that it maintains a number of
free lists for different size classes of objects, with a shared pool of pages
that can be acquired and released by these free lists).
