
\section{Effects of DRAM Limitations}
In this experiment, we quantify the extent to which specific DRAM limitations
affect execution time. We started with the validated model, \emph{Full}, and
gradually stripped out features: \emph{No Refresh} disables refresh, \emph{No
ACT Limits} sets $t_{FAW}$ and $t_{RRD}$ to zero, finally, \emph{Ideal},
removes all other timing considerations except $t_{CS}$, $t_{RCD}$ and
$t_{RP}$\footnote{Specifically, $t_{RTP}$, $t_{WTR}$, $t_{RTRS}$, $t_{RTP}$ are
set to 0; $t_{RAS} = t_{RCD} + t_{CS}$ $t_{RC} = t_{CS} + t_{RCD} + t_{RP}$.}.

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\columnwidth]{figures/dram_fidelity_speed_slowdown.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\columnwidth]{figures/dram_fidelity_rate_slowdown.pdf}
    \end{subfigure}
    \caption{SPEC2017 target-execution time with various degrees of modeling fidelity.}
    \label{fig:model-fidelity-sweep}
\end{figure*}

The slowdowns of these models relative to a single-cycle memory system is shown
in figures~\ref{fig:model-fidelity-sweep}.

Unsurprisingly, model fidelity becomes more important when the memory system is
loaded; only refresh meaningfully affected runtime~\TODO{give a figure}. For
intrate, refresh-induced slowdown grows and is workload dependent.  Eliminating
$t_{RRD}$ and $t_{FAW}$ had no effect: we suspect reducing rank count and
increasing device density, or using a target that generates more memory
requests would induce these effects. Remaining DRAM non-idealities do
contribute a \TODO{slowdown}, except in perlbench and exchange. One possible
explanation is that the MAS scheduling CASW commands earlier than it would
otherwise; late arriving reads must wait on inflight writes instead of being
scheduled ahead, increasing average read latency, even though data-bus
utilization is locally greater.

\section{Working Set Study}
Caches help insulate processors against long and variable latencies to DRAM;
unfortunately, large LLCs are difficult to implement directly on FPGAs due to
area limitations. \PNAME enables the architect to explore cache sizes that
would otherwise be impossible to host on a single FPGA. We explore this by
sweeping LLC-cache sizes from \wunits{64}{KiB} to \wunits{64}{MIB} in size. In
figure~\ref{fig:llc-speed} we show, the speed up different cache sizes offer
and constrast them against a cacheless and single-cycle memory-system.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/cache_size_bar_plot.pdf}
    \caption{LLC-induced speedup in SPEC2017 intspeed vs a cache-less DRAM model.}
    \label{fig:llc-speedup}
\end{figure}

\section{Simulation Performance}

\input{tex/tables/simulator-performance.tex}

We built \PNAME to bridge the gap between detailed-but-slow software
cycle-accurate DRAM simulators, and fast-but-inaccurate abstract FPGA-models.

Tables\ref{tbl:intspeed-simtimes} and \ref{tbl:intrate-simtimes} give the
host-times and execution speeds for a handful of SPEC2017 runs. The targets
with DDR3 models consistently above \wunits{100}{MHz}. Theoretically, our
models can operate at unity FMR~(160 MHz), the target memory-systems we were
are consistenly faster than the host-memory system.  In this regard, a
single-cycle memory system represents the worst case --- a point made made
clear in the tables. On our host, reads and writes take on average\TODO{} and
\TODO{} when unloaded.  We can attribute at least 10 cycles of round-trip delay
to move between the instance and host-DRAM controller, due to asynchronous
FIFOs, width adapters, and inserted pipeline stages.
