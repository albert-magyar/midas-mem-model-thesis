To avoid confusion when speaking of computers simulating computers, the
literature commonly makes a distinction between the \emph{target}, the computer
being simulated, and the \emph{host} or \emph{host-platform}, the computer
executing the simulation. The host-platform is often not a single machine but
a collection of interconnected machines, which may include CPUs,
GPUs, and FPGAs.

\section{A Tour of Full-System Simulation}

Simulation performs three different functions.

\begin{enumerate}

    \item \textbf{Prototyping:} ``What thing should we
        build?" Prototyping serves as a means to rapidly evaluate different
        design points with an imperfect model of a proposed design.

    \item \textbf{Verification:} ``Did we build the thing right?" Verification
        serves to check, or prove, that a particular implementation
        correctly executes.

    \item \textbf{Validation:} ``Did we build the right thing?" Validation
        serves to show that the implementation fulfills the objectives set out
        for the system.

\end{enumerate}

Both prototyping and verification can be applied at all levels of the design
hierarchy.  For example, given a specification of the system into which an
accelerator is integrated, one could prototype different design points and
verify an implementation of that accelerator. Validation, however, seeks to
answer a system-level question that spans the entire computing stack.  The
surest way to validate a system is not in simulation, but at-speed with a
physical prototype or the final product itself. This pushes validation late
into the design cycle. To perform, \emph{pre-silicon} validation a fast and
accurate full-system simulator is required.

Generally, there are only two points during the development of an SoC where
full-system simulation executes within even three orders of magnitude of a
silicon implementation. Very early, when architecture-level simulators, like QEMU\cite{qemu}, are used
to perform initial system prototyping (hundreds of MIPS). And very late: when
the SoC is completely implemented and full-system emulation tools like a
Synopsys Palladium~\cite{palladium} (ones of MHz) can be used -- if they are
available.

When a hardware-emulation platform is unavailable or too expensive, and when
faster full-system simulation is desired earlier, it is common in industry to
use FPGA prototypes. An FPGA prototype directly implements the SoC on
one more more FPGAs, often with a custom board design that may include
peripherals identical to those that would be deployed in the final system. FPGA
prototypes are fast enough to support software development, they execute at ones to hundreds of
MHz, and thus provide a greater degree pre-silicon validation. Moreover, for large
projects, they are inexpensive enough to duplicated and shared between hardware
and software engineering teams.  The difficulty with using FPGA prototypes as
full-system simulators is that they require complete RTL implementation of the
design (and, sometimes, a custom PCB). While FPGA prototypes can accelerate
software development by months, they become useful too late in the design cycle
to do hardware-software co-design, as the target hardware cannot easily be
modified, limiting the scope of potential changes.

Since a complete RTL implementation precludes the use FPGA prototypes for
design space exploration and early prototyping, they are generally unused in
academia. Instead, full-system simulation is done entirely in software with
simulation frameworks such as Gem5~\cite{gem5}, and MARSSx86~\cite{marssx86}.
These simulators can run target workloads at up to hundreds of KIPS, but are
often much slower in practice when employing detailed or custom models. This
makes it practically impossible to run complete workloads, such as
multi-threaded Java applications or SPECint2006~\cite{spec} with its reference
inputs. A common remedy is to employ statistical sampling
techniques~\cite{smarts} to fast-forward to the region of interest, before
executing O(100M) instructions at the desired fidelity.

While this approach has well-acknowledged shortcomings~\cite{gem5error},
judicious use of cycle-level simulators can be an appropriate vehicle for
proposing new microarchitectural ideas. But for radical proposals that involve
aggressive microarchitectural changes or traverse multiple layers of the
computing stack, this approach is inadequate (particularly for workloads that
are long-running, irregular and require a large number of cores, such as
managed-language workloads~\cite{MicroSimPanel}).

The ideal full-system simulator -- sufficient for both academic and industrial
uses -- would be inexpensive, fast and as accurate as desired throughout the
whole design process; it would always possible to run the software stack on a
model of the target hardware at speeds fast enough for software development.
Initially, this simulator could be used for system-level prototyping and design
space exploration, but as desired, more detailed models or RTL implementations
of target components would be integrated. Ultimately, once all of the target
RTL has been integrated, the simulator subsume the role of an FPGA prototype.
Academics need not completely implement an SoC; they may simply stop adding
fidelity to the simulation once they are content with the quality of their
results.

Presently, FPGAs are the only commercial off-the-shelf (COTS) technology
capable of supporting fast, scalable, cycle-accurate simulation. Thus, we
believe any attempt to build this ideal simulator must necessarily use FPGAs.
However, this requires a more flexible perspective of how FPGAs can be deployed as
simulation \emph{accelerators} and not merely RTL emulation devices as they are
used in FPGA prototypes.

\section{Defining FPGA-Accelerated Simulation}

The distinction between FPGA-accelerated simulation and FPGA prototyping can be
nebulous: we argue that FPGA prototyping represents a narrow subset of the larger
space of FPGA-accelerated simulation. Key to understanding the distinction is
first, think of simulation as any other application executing on a host, and
second, forget that a host may be or include one or more FPGAs.

A simulation is an application that takes an input and produces an output.  The
output may be the console or file I/O of the target.  Alternatively, it may be
a dump of the microarchitectural state of the target as it changes over the
lifetime of the simulation.  As far as the user is concerned, the simulation
may be optimized in any way whatsoever so long as this output is the same.

Like any other application, simulations have hot spots that account for the bulk
of their runtime. To improve runtime, the simulation may be parallelized over
the host, either over multiple homogeneous resources, or by offloading specific
kernels to accelerators, which may execute concurrently or with the rest
of the application.

%In simulations that account for time at the cycle-level, much of this runtime
%is dedicated to modeling the cycle-by-cycle interactions of parts of the
%system. These models may be written in C++ or SystemC, or implemented in an HDL
%like verilog or VHDL. Parts of the simulation may be divided into functional
%models, like an architecture simulator, and a timing-model that does some
%accounting of time based on a model of the microarchitecture. What's important
%to note, is that any cycle-level model of hardware attempts to capture the
%behavior of a fine-grained highly concurrent digital-circuit. Taken to the
%limit, a cycle-level model is an RTL model.

In FPGA-accelerated simulation, in general, we attempt to offload parts of the
simulation that do cycle-level or cycle-accurate modeling of some part of the
target. One way to achieve this is to dissolve the simulation spatially
(perhaps along module boundaries): parts of the target, like an NoC or Core
pipeline, could be offloaded to an FPGA, while models for I/O may be hosted on
a CPU.  Alternatively, one could host a functional model of a module on the CPU
and accelerate the timing-model on the FPGA (or vice versa).  Again, the only
constraint on the implementation of the simulation, and thus, the
implementation of FPGA-accelerated components of the simulation, is that the
output of the simulation is the same. Once this constraint is met, a faster
implementation is always better.

\section{Prior Work in FPGA-accelerated Simulation}

Researchers have devised many clever techniques for using FPGAs as simulation
accelerators. To highlight some of these techniques, we summarize the FAME
taxonomy~\cite{fame}, which outlines three dimensions along which to categorize
FPGA-accelerated simulators.

\subsection{FAME1: Host-Target Decoupling}

In host-decoupled fpga-accelerated simulators, a target-cycle of simulator
executes over a variable number of FPGA-host cycles, in contrast to an
FPGA-protoype where a single target-cycle is executed on every FPGA-host cycle.
With this technique, multi-ported register files and CAMs can be modelled using
dual-ported BRAM, saving FPGA resources. Additionally, host-decoupling permits
the simulator to tolerate variable latencies in the host-platform without
sacrificing simulator performance or changing the target-time behavior of the
simulator. Nearly all academic FPGA-accelerated simulators are FAME-xx1
simulators, though ProtoFlex~\cite{protoflex} is an early example of one.

\subsection{FAME2: Multithreading}

In a multithreaded FPGA-accelerated simulation, multiple virtual instances of a
block or module within the target, are simulated using the same datapath on the
FPGA -- the target state is duplicated according to the number of virtual
instance. The intiution here is that, ASIC logic tends to be expensive when
mapped to FPGA fabric; in FPGA-prototypes, designs tend to be logic~(LUT)
constrained, which leaves much of the FPGA's embedded BRAM left unused.
Multithreading improves the mapping efficiency of the target, by reusing the
expensive logic over multiple copies of target state which maps easily into
FPGA state. HASim~\cite{hasim} is an example of a multithreaded simulator.

\subsection{FAME4: Abstraction}

In an abstract FPGA-accelerated simulation, components of the simulator do not
model the RTL exactly. For example, instead of having a complete model of the
DRAM-subsystem, an abstract model that reponds to requests after a fixed
latency may be used. Abstraction permits simply components of the simulator,
trading simulation fidelity for FPGA resources.

\subsection{Split Timing-Functional Models}

Splitting timing and functional models is ubiquitous both in software and
FPGA-accelerated simulators, as it permits amortizing the design effort of the
functional model over multiple timing models. This is especially crucial in
FPGA-based simulation, where the functional model can account for the bulk of
the complexity (e.g., 35K vs. 1K SystemVerilog LoC in RAMPGold~\cite{rampgold}. With
host-decoupling, it is possible to host the two models on different parts of
the host-platform: FAST~\cite{fast} hosted its functional model in software and its
timing model in fabric (ProtoFlex did the opposite~\cite{protoflex}).

For a comphrehensive survey of FPGA-accelerated simulation work we direct
the reader to~\cite{fpgasimbook}.

\section{Adoption Challenges}

Despite their promises, FPGA-accelerated simulators have only been employed by
the researchers that developed them. The failure to adopt FPGA-accelerated
simulation methodologies more widely comes as a result of several key factors:

\begin{enumerate}

    \item \textbf{Availability.} Much of the early FPGA simulator research
        relied on boutique FPGA-host platforms like the BEE~\cite{bee2}, or
        used custom board designs. The cost of these platforms disincentivizes
        their adoption by researchers who already have the means to run
        software simulations at low cost.

    \item \textbf{FPGA Capacity.} Common ASIC structures, such as CAMs,
        multi-ported RAMs, and wide multiplexors are known to map poorly to
        FPGA fabrics~\cite{fpgagap, fpgagap2}, making it difficult to host
        large target designs on an FPGA.

    \item \textbf{Configurability \& Extensibility.} Extending FPGA-accelerated
        simulations requires writing RTL. RTL models are less configurable and
        harder to extend than software models. Finally, RTL models still need
        to be validated, further exacerbating the challenge of building them.

    \item \textbf{FPGA compile time.} Compiling an FPGA simulator takes many
        orders of magnitude longer than compiling a software simulator (ones of hours).
        %To some extent this is inescapable. However, where abstract models are
        %employed, they can be made run-time configurable, with programmable
        %registers sitting on a simulation memory map. Where models are
        %generated from RTL, they can can be incrementally recompiled, or perhaps
        %partially reconfigured.

    \item \textbf{Debuggability.} Debugging a broken FPGA-accelerated
        simulation is difficult due to the limited visibility the designer has
        over the state of the simulation. This is often more challenging than
        debugging a an FPGA prototype of the target, as FPGA-specific
        optimizations make it more difficult to reason about the state of the
        target.

\end{enumerate}

\section{Why Revisit FPGA-Accelerated Simulation?}

Even as Moore's law wanes, FPGA capacity continues to scale. The largest FPGAs
have over 50 MB of BRAM and millions of logic cells\footnote{Comically, scaling
RAMPGold~\cite{rampgold}, to use the largest Xilinx UltraScale
FPGA~\cite{ultrascale} by BRAM capacity would permit modeling in excess of 5000
cores.}. As they have scaled, FPGAs have continued to become more
heterogeneous, adding features that make them more amenable to hosting
full-system simulators.  Both Intel and Xilinx now sell FPGAs with embedded ARM
cores, making it easier to co-simulate tightly coupled hardware and software
models of a system. Modern FPGAs include hardened DRAM controllers that are
comparable to those of ASICs. This trend towards greater integration looks to
continue. For example, upcoming Intel Stratix 10 MX parts include in-package DRAM (HBM2)
that can support up to 1 TBps of aggregate memory bandwidth~\cite{stratix10mx}.
Both DRAM capacity and bandwidth are crucial for simulating components
of the target that may not fit in BRAM.

Lower cost and increased on-chip integration have also made FPGAs more
accessible. Not only are COTS development boards cheaper and more featureful,
FPGAs are now available in academic clusters, like TACC's Microsoft
Catapult~\cite{catapultannounce} deployment, and in datacenters, as a service
like Amazon Web Services' EC2 F1 instances~\cite{amazonf1}. Where in the past
academics would have to purchase their own FPGAs -- even to reproduce published
experiments -- it may soon be possible for them to instead spin up an identical
simulation on a shared computing resource. Companies may no longer have to
maintain their own FPGA prototyping clusters; they could instead batch out
simulations to the cloud.

\section{Improving Usability Through Automation}

While the trends described in the previous section solve the
\emph{availability} and \emph{FPGA capacity} challenges, the remaining three
usability challenges persist. Previous work~\cite{fabscalarfpga, strober} has
shown that much of an FPGA-accelerated simulation can be automatically
generated from source RTL. This RTL can be written in an HDL like Verilog or
emitted by high-level synthesis tools or generators written in languages like
Chisel~\cite{chisel}. While this still requires an RTL implementation, the same
RTL can be passed to an EDA flow to measure physical design metrics, and no
validation of the generated model is required.

This is, unfortunately, not a panacea: perhaps the RTL is not yet
available, or a more abstract, reconfigurable model is desired. In this report
we consider off-chip DRAM memory systems as a motivating example: they have too
much state to be hosted in fabric and yet they must must be tightly coupled to
the processor model making it difficult to co-simulate DRAM in
software.\footnote{High-latency peripherals, like disks, can often be modeled
in software without any performance cost~\cite{disksim}.} These components
typically require an abstract model that virtualizes the target-memory system
over FPGA-host-memory system.

This reintroduces the aforementioned problem that anything but a simplistic RTL
model is difficult to design, modify and reuse. We propose to address this
through \emph{generators} that synthesize abstract memory system models that
can be easily modified and used across a wide range of targets.  This generator
must provide a variety of timing-models so as to enable the designer to trade
fidelity for FPGA area when needed. Generated models must be reconfigurable, to
permit sweeping memory system parameters without needing to recompile the FPGA
bitstream. These models must provide useful instrumentation to both aid in
debugging and to provide insight about memory system behavior without
perturbing execution. Finally, when the generator does not provide an
appropriate timing, the generator should be easy to extend.
